---
title: "Introduction to SDMs"
author: "zoontutorials team"
date: "6 February 2017"
output:
  rmarkdown::html_vignette:
    toc: yes
vignette: >
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteIndexEntry{Introduction to SDMs}
  %\VignetteEncoding{UTF-8}
---

# Introduction
## Why do Species Distribution Models?
In ecology, we often want to understand where a species is located in the environment and the restraints on that distribution so we can predict where the species may occur in the future. We typcially consider that species exist within their "ecological niche." In contrast to geographic space, the ecological niche does not describe a physical place, but rather the environmental conditions required for a species. 

Ecologists typically consider species' ecological niche space to be constrained by three dominant factors: the abiotic environment, the biotic environment, and movement. Characteristics of the ecological niche include the abiotic and biotic habitat, the resources it obtains from the habitat, and the species' activity pattern that allow it to utilise its space. Examples of these include the climate required for a species to survive, the local vegetation types it depends on, and it's abillity to traverse different landscapes, respectively. The intersection of the abiotic and biotic environmental conditions constitutes the "fundamental niche," or where the species could physiologically occur given what we know of its biology and ecology. Experience tells us, however, that species are not always located where we expect them to be. Their "realised niche" is where they actually exist within the bounds of the environmental space where they could potentially exist. This is conceptualised by the bounds of the species' activity or movement, which may restrict to it where it can access. Invasive species are often those that are introduced to a new place by anthropogenic activity, thereby allowing them to overcome a movement barrier, and subsequently thrive in a region that exists within their fundamental niche. 

## Types of SDMs
There are two main types of SDMs: mechanistic and correlative. Mechanistic models are considered a "bottum-up" approach to modelling species distributions because they begin with mathematical formulations of the specific environmental and biologic processes that impact a species' distribution and scale up to predict occurrence in geographic space. Parameters included in this type of model have physical meaning, such as thermotolerance thresholds, and we use that understanding to expand to predictions at a larger scale. 

Correlative models, which are the chief concern of the `zoon` workflow, are a "top-down" approach. These models lack a detailed mathematical formula to explain the physical processes underlying species' occurrence, and rather use statistical models to describe occurrence in relation to selected environmental and biological parameters. Correlative models use species occurrence data in combination with environmental preictors to use underpinning statistical models that help us derive new maps of the likelihood of species occurrence. 

When we build SDM's, we are trying to describe the ecological niche of a species using statistical correlations with environmental predictors. The statistical models we use, therefore, have implications on the how generalisable the resulting conclusions are. We have to make trade-offs between how well our model will fit our data, and how applicable the results will be to the species in general. Correlative models are typically better at characterising the realised niche of a species, rather than the fundamental niche, because of the type of data they incorporate. 

# Steps in a correlative SDM
The basic steps of a correlative species distribution model are:
    1. Occurence: Prepare and format species occurrence data 
    2. Covariates: Preapre and format the environmental predictors you will use in your model
    3. Process: Make any modifications and adjustments to your data sets
    4. Model: Fit statistical models
    5. Output: Produce either predictions or ecological inferences, for example in the form of maps

The `zoon` `workflow` is designed to make building and fitting correlative SDMs based on the above steps as straight-forward and reproducible as possible. To fit an SDM you use the `workflow` function, and need only to select a module in each of the five main steps of a correlative SDM: occurrence, covariate, process, model, and output. The following sections of this tutorial will guide you through the process of selecting a module for each argument of the `workflow` function. But first, don't forget to load the package!

```{r message=FALSE, warning=FALSE}
library(zoon)
```
A basic workflow, in this example for the mosquito *Anopheles Plumbeus* in the UK, might look like this:  
```{r message=FALSE, warning=FALSE, fig.align='center', fig.height=4, fig.width=3}
UKMosquitoes <- workflow(occurrence = UKAnophelesPlumbeus,
                             covariate = UKAir,
                             process = OneHundredBackground,
                             model = RandomForest,
                             output = PrintMap)
```

`zoon` comes with several pre-existing modules for each argument, which we will go through in more detail in the rest of this tutorial. You might like to explore some different combinations yourself, and you can find more information by running the `GetModuleList` command.


# Zoon SDM Workflow
## Step 1. Occurrence
SDMs are most frequently fit with species occurrence data (rather than, for example, abundance data), and this can come in three formats: presence-only, presence-background, and presence-absence. 

* Presence-only data is usually sourced from museum or herbarium records, and is a list of all recorded sightings/captures of a species with its location. 

* Presence-background data is a type of presence-only data that combines presence records with a set amount of randomly generated background points, also called "pseudo-absences." This is the more common of the two types. 

* Presence-absence data generally comes from structured field surveys where the presence *or* absence of the target species is recorded for a given site.

In practice, presence-only data is more common than presence-absence data, so this tutorial is written for presence-only data as default, and any necessary alterations for presence-absence will be noted.

The first module required in a `workflow` is `occurrence` and it is where you load your species occurrence data. There are three methods for loading data into your workflow depending on where your data come from: you can use pre-existing `occurrence` modules in `zoon`, download data from an online repository, or load in your own data from a local computer. All three methods are demonstrated below. 

### Existing zoon data
`zoon` comes with several pre-existing dataset modules that you can use. These are available with the `GetModuleList()` command. Under the `$occurrence` sub-heading you can see all available `occurrence` modules including these datasets. For this tutorial, we will be using the `CarolinaWrenPO` module, which is a presence-only dataset of the Carolina Wren in the USA. This is a common species in in North America whose range reaches from southern Canada to north-eastern Mexico, but the dataset has been cropped to the extent of the contiguous USA. To use this dataset, or any other module available within `zoon` for your `occurrence` data, we use the following line of code inside the `workflow`:

```{r eval=FALSE}
occurrence = CarolinaWrenPO
```

### Data from online repositories
You may want to source your species occurrence data from online repositories. Several online repositories are available to use [hint: use `ModuleHelp(“SpOcc”)` to see a list of options], 

The `SpOcc` module will allow you to access data from these databases. This module requires a number of specifications: species, extent, database, type, and limit. Species refers to the scienfic name of your species of interest, extent describes the area you are interested in, database requires the name of your repository, type denotes the type of occurrence data you are interested in (for example presence-only), and limit corresponds to the number of records to be sourced. 

In this example we will use GBIF. We are interested in obtaining presence-only data for the Giant Panda, so we set our `occurrence` module like this:

```{r eval=FALSE}
occurrence = SpOcc(species = “Ailuropoda melanoleuca”, extent = “ ”, database = “GBIF”) 
```

** Needs finishing when GBIF is working again to test **

### Loading your own data
If you have your own occurrence dataset from field surveys that you want to analyse, then you can load it using the `LocalOccurrenceData` module. This requires a .csv file with three columns for the values of longitude, latitude, and value of species presence, in that order. For alternate coordinate systems refer to the information about a CRS column using `ModuleHelp("LocalOccurrenceData")`. 

To load your own presence-only dataset, set your `occurrence` module within `workflow` as follows:

```{r eval=FALSE}
occurrence = LocalOccurrenceData(filename = "myData.csv",
                                 occurrenceType = 'presence')
```

To load your own presence-absence dataset, set your `occurrence` module within `workflow` as follows:

```{r eval=FALSE}
occurrence = LocalOccurrenceData(filename = "myData.csv",
                                 occurrenceType = 'presence/absence')
```

**Add example here**

`zoon` has several accessor functions [NOTE: THIS IS UNCLEAR TO A BEGINNER] available to view the output of each module in your workflow. You can view your raw species occurrence data using the `Occurrence()` function:

```{r eval=FALSE, message=FALSE, warning=FALSE}
Occurrence(Your_Workflow_Name_Here)
```

The outputs of this accessor function are shown below for both the `CarolinaWrenPO` (top) and `CarolinaWrenPA` data modules (bottom). The first two columns provide the geographic location of the observation (as longitude and latitude), the third column is the observation value (1 = presence, 0 = absence), the fourth column is the type of observation, and the last column identifies the "fold" the data point is located in if utilising cross-validation (covered later in the section of `Process` modules, but a default model can be considered to always have one fold).

```{r eval=TRUE, echo=FALSE, message=FALSE, warning=FALSE}
hidden_PO_workflow <- workflow(occurrence = CarolinaWrenPO,    ### just some basic workflows so there
                             covariate = CarolinaWrenRasters,  ### is something to run accessor functions
                             process = OneHundredBackground,   ### on as examples
                             model = MaxEnt,
                             output = NoOutput)

hidden_PA_workflow <- workflow(occurrence = CarolinaWrenPA,
                             covariate = CarolinaWrenRasters,
                             process = NoProcess,
                             model = LogisticRegression,
                             output = NoOutput)
```

```{r eval=TRUE, echo=FALSE, message=FALSE, warning=FALSE}
head(Occurrence(hidden_PO_workflow))
```

```{r eval=TRUE, echo=FALSE, message=FALSE, warning=FALSE}
tail(Occurrence(hidden_PA_workflow))
```

## Step 2. Covariates

The second module required in a `workflow` is for your environmental data. As with the `occurrence` module, there are three ways to load `covariate` data into your model: pre-existing `zoon` modules, data from online sources, and your own data from a local computer.

### Existing *zoon* data

You still find the pre-existing `zoon` `covariate` modules using `GetModuleList()` under the `$covariate` sub-heading. For this tutorial, we will be using the `CarolinaWrenRasters` module. This is the complementary environmental data to our chosen `occurrence` module. To use this dataset, call your `covariate` data in the `workflow` as follows:

```{r eval=FALSE}
covariate = CarolinaWrenRasters
```

### Data from online repositories

The three main `covariate` modules for sourcing environmental data from an online repository are `NCEP`, `Bioclim`, and `Bioclim-future`.

* `NCEP`: This module will grab coarse environmental data from the National Centers for Environmental Prediction (NCEP). This requires you to set the coordinates of the model extent and provide a character vector of which variables to select (use `ModuleHelp("NCEP")` to see the options).

```{r eval=FALSE}
covariate = NCEP(extent = c(-5,5,50,60), # default values for the NCEP module
                  variables = "hgt")   
```

* `Bioclim`: This module will grab bioclimatic variables data from WorldClim. This requires you to set the coordinates of the model extent, select a resolution for the data (in minutes), and a vector of integers to identify which bioclim variables to source. Refer to [WorldClim](http://www.worldclim.org/bioclim) for a list of available bioclim variables.

```{r eval=FALSE}
covariate = Bioclim(extent = c(-180,180,-90,90), # default values for the Bioclim module
                     resolution = 10,
                     layers = 1:5)   
```

* `Bioclim_future`: This module will grab biolimatic variables data from WorldClim for the future (CMIP5). This requires you to set the coordinates of the model extent, select a resolution for the data (in minutes), a vector of integers to identify which bioclim variables to source, a [Representative Concentration Pathways trajectory](https://en.wikipedia.org/wiki/Representative_Concentration_Pathways), a General Circulation Model **(need a link!)**, a time period for the projection ('50' for the period 2041-2060, or '70' for 2061-2080).

```{r eval=FALSE}
covariate = Bioclim_future(extent = c(-10, 10, 45, 65),   # default values for the Bioclim_future module
                            resolution = 10,
                            layers = 1:19,
                            rcp = 45,
                            model = "AC",
                            year = 70)
```

### Loading your own data

You can load your own environmental data into your `workflow` uing the `LocalRaster` module. This will load in either a single raster or a list of rasters. If you load a list of rasters, the module will create a raster stack in the form of a string of the raster layer filenames, a list/vector of strings of raster filenames to be stacked, a RasterLayer, or a RasterStack object. The structure of this module call is:

```{r eval=FALSE}
covariate = LocalRaster(rasters = "MyRaster")
```

Using the `Covariate()` accessor function will show you important details of your raster stack, including the extent of the dataset, the coordinate projection scheme (here, WGS84), and minimum/maximum value for your variables. 

```{r eval=FALSE}
Covariate(Your_Workflow_Name_Here)
```

```{r eval=TRUE, echo=FALSE, message=FALSE, warning=FALSE}
Covariate(hidden_PO_workflow)
```
You can use the `?'RasterStack-class'` command to find a more detailed summary of your data. 

## Step 3. Process
Now that you have loaded in your species and environmental data using the `occurrence` and `covariate` modules, the `process` modules will perform any modifications required before fitting the model itself. These modules can be broadly classified into those that alter your raw data, those that generate additional data, or those that set up model validation.

### Modify your data
`process` modules that modify your data improve interpretability of your results. They can involve, for example, tidying or transforming your raw data or standardising your covariates.

*	Inaccurate data can lead you to draw false conclusions, so the `Clean` module removes impossible, incomplete, or unlikely occurrence points in a dataset based on longitude/latitude values. Use `ModuleHelp("Clean")` to see how to select a cleaning method.

```{r eval=FALSE}
process = Clean(which = c(1,2,3,4))   # default fit of the Clean module
```

*	The `Transform` module lets you apply a transformation function (e.g. square or log) to your environmental variables. You might do this to [[INSERT EXPLANATION HERE!]. To use this module you will need to define the transformation, provide a character vector listing the covariates to be transformed, and state whether you want to replace the original variable with the transformed version or add it as an extra layer.

```{r eval=FALSE}
process = Transform(trans = function(x) {x},   # default form of the Transform module. Perform no transformations on any variable
                    which_cov = NULL,
                    replace = TRUE)
```

```{r eval=FALSE}
process = Transform(trans = function(x) {x^2},   # Add a new variable that is the squared transformation of the "cov1" variable
                    which_cov = "cov1",
                    replace = FALSE)
```

```{r eval=FALSE}
process = Transform(trans = function(x) {log(x)}, # Replace the "cov1" variable with the log-transformed version of "cov1"
                    which_cov = "cov1",
                    replace = TRUE)
```

*	The `StandardiseCov` module standardises covariates in your model, improving comparison of effect size of variables. By default, the module standardises all variables, and does so by subtracting the mean and dividing by the standard deviation. This standardisation places variables on the same scale. For example, the regression coefficient for the distance of a site to roads might be +3.0 when measured in kilometres, but +0.003 if measured in meters, and the effect of average temperature could be -10C. How would you compare the effect of these variables? Standardisation allows you to compare the relative effects of different covariates within a model. To use this module you need to choose which variables to exclude from standardisation (if any), and whether to use the Gelman variant (standardises by two standard deviations instead of one - **(link to this here)**.)

```{r eval=FALSE}
process = StandardiseCov(Gelman = FALSE, exclude = NULL)   # default form of the StandardiseCov module.
```


### Generate new data

`process` modules that generate new data include those that create background data points for models fit to presence-only data, or introduce interactions between variables in the model.

All models that utilise presence-only data sets **(or almost all? at least within zoon)** require the generation of background data points (aka pseudo-absences). [WE MAY NEED A DISCUSSION HERE ABOUT WHY WE GENERATE PSEUDO-ABSENCES]
These data points represent the range of environmental conditions present in the region being modelled. There are several modules in zoon that undertake this process: 

* `Background` generates n background samples (default 100). You need to define the number of background samples you wish to generate, and have the option of defining a bias value (see `ModuleHelp("Background")`).

```{r eval=FALSE}
process = Background(n = 100,   # generate 100 unbiased background points
                     bias = NULL)
```

* `OneHundredBackground` and `OneThousandBackground` generate 100 and 1000 background points, respectively.

```{r eval=FALSE}
process = OneHundredBackground
```

```{r eval=FALSE}
process = OneThousandBackground
```

Interactions between variables can have important implications for the interpretation of statistical models. Expand when back in front of textbooks (Something about interpretation of response curves? Doesn't effect maps). The `addInteraction` module lets you define interactions between variables in your model. This is not generating new data per se, but adds additional variables to the model based on existing variables. There are three ways to implement the `addInteractions` module:

* You can add all pair-wise interactions:
```{r eval=FALSE}
process = addInteractions(which.covs = 'pairs')
```

* You can add all interactions between a select group of variables:
```{r eval=FALSE}
process = addInteractions(which.covs = c(A,B))   # adds an interacton between A & B

process = addInteractions(which.covs = list(c(A,B), c(A,C)))   # adds interactions between A & B and A & C, but not B & C

process = addInteractions(which.covs = c(A,B,C))   # adds all two-way (e.g. A & B) interactions and a three-way interaction between A, B & C
```

* You can specify polynomial terms:
```{r eval=FALSE}
process = addInteractions(which.covs = c(A,A))   # leads to a quadratic polynomial

process = addInteractions(which.covs = c(A,A,A))   # leads to a cubic, polynomial
```

### Set up model validation

Model validation is a very important step in the model fitting process as it lets you determine if your model is an acceptable description of your data. Model validation can be as simple as including goodness-of-fit tests or analysing the residuals **can this be done for SDMs?** but the `zoon` package uses more complex methods that subset and/or resample the dataset. **[Expand on what model validation is/why it is important]** The `process` modules that set up model validation include:

*	`Crossvalidate`: Run k fold crossvalidation (default = 5). If you are using presence-absence, this will split presences and absences separately so folds have equally balanced data, otherwise it just samples. This samples the dataset without replacement into k 'folds' or sub-datasets to be used for training and testing the model. A model is fit to the k-1 folds of training data and evaluated against the fold of testing data (and repeated so each fold is used as the testing data once, and then the results are averaged). This is mainly used where the goal is to use the model for prediction as it tests if the model is overfit to the training data and thus unlikely to be suitable for prediction across another dataset (i.e. the test fold). 

```{r eval=FALSE}
process = Crossvalidate(k = 5)   # 5-fold cross-validation
```

*	`Bootstrap`: This module generates random bootstraps of the data. **expand** It defaults to the number of points in the dataset

```{r eval=FALSE}
process = Bootstrap   # Bootstrap a number of points equal to the number of points in the dataset
```

*	`BackgroundAndCrossvalid`: This module generates up to 100 background records at random in the raster cells and splits all data in k folds for cross validation (default = 5).

```{r eval=FALSE}
process = BackgroundAndCrossvalid(k = 5)   # generate up to 100 background points, and the split all data into 5 folds for cross validation
```

In some instances you may not require a `process` module in your `workflow`,  however, it is a mandatory argument and so the `NoProcess` module can be used as a 'blank' module.

```{r eval=FALSE}
process = NoProcess
```

## Step 4. Model

Arguably the most important part of your `workflow` is the `model` module. This is where you choose which type of SDM you want to fit to your data. There are many different SDM methods to choose from **(add Elith paper reference)** and each have their mertis, but here we provide a quick overview of some of the more common methods. 

### Logistic Regression

Logistic regression is a type of Generalised Linear Model (GLM) that can be fit to presence-background and presence-absence datasets. It is best suited for these datasets where the outcome is a binary variable (presence represented as "1" and absence as "0," for example) that is dependent on one or more independent explanatory variables. Coefficients are estimated by choosing parameter values that maximise the likelihood of observing the sample values. You select `LogisticRegression` model by defining your `workflow` as follows:

```{r eval=FALSE}
model = LogisticRegression
```

### Generalised Additive Model

In contrast to GLMs, in which the coefficient for each covariate is estimated, in Generalised Additive Models (GAMs) the linear predictor is the sum of smoothing functions fit to each measured variable. **(Add definition of smoothing functions?)**. This model can be fit to presence-background and presence-absence datasets. The `mgcv` module fits a GAM using generalised cross-validation via the `mgcv` package. To fit this model you need to define a basis dimension used to represent the smooth term, and a two-letter character string indicating the smoothing basis to use. You select this model in your `workflow` as follows:

```{r eval=FALSE}
model = mgcv(k = -1,   # default settings
             bs = "tp")
```

### MaxEnt

MaxEnt is the most widely used SDM algorithm **(Elith reference)** today. This is a machine learning method that computes a probability distribution over the environment data grid cells, and the chosen distribution is the one that maximises the entropy (hence, MaxEnt), subject to some constraints. This method works with presence-background data. The `MaxEnt` module uses the `maxent()` function in the `dismo` package, and requires a MaxEnt executable file saved in the correct location. The `zoon` helper function `GetMaxEnt()` is available to help with this installation. You select this model in your `workflow` as follows:

```{r eval=FALSE}
model = MaxEnt
```

### Boosted Regression Trees

Boosted regression trees (BRTs; also known as Gradient Boosting Machine, or GBM), are a machine learning technique that produces a prediction model in the form of an ensemble of weak prediction models (i.e. decision trees). This differs from the standard regression approach of fitting a single best model by using the "boosting" technique to combine relatively large numbers of simple trees adaptively, optimising predictive performance. The `GBM` module fits a generalised boosted regression model using the `gbm` package. For this model you need to define the maximum number of trees, the maximum depth of variable interactions, and a shrinkage parameter (aka the learning rate). This model can be fit to presence-background and presence-absence data using the following call in your `workflow`:

```{r eval=FALSE}
model = GBM(max.trees = 1000,   # default values
            interaction.depth = 5,
            shrinkage = 0.001)
```

### RandomForest

Similar to the boosted regression trees in the `GBM` module, random forests are a machine learning technique that make use of an ensemble of weak prediction models (i.e. decision trees). Random forests differ from BRTs by fitting each decision tree independently from previously fit trees using bootstrapped samples of your data. **(need a better definition of the difference between BRT and RF?)**. The `RandomForest` module can be fit to presence-background or presence-absence data using the following call in your `workflow`:

```{r eval=FALSE}
model = RandomForest
```

## Step 5. Output
* types of output
* interpreting outputs

## Putting it all together

Now that we have seen how to use each argument in the `workflow` function it is time to put all of the pieces together and fit our own SDM within `zoon`. Since MaxEnt it is the most popular SDM methodology, we will fit a MaxEnt model to our Carolina Wren presence-background data set, generate 1000 background samples, and display our results in a map without our datapoints displayed. We have covered each of the necesssary modules previously, and here we will run them together to form a complete `workflow`.

```{r eval=TRUE, message=FALSE, warning=FALSE, fig.align='center', fig.height=7, fig.width=7}
myMaxEntWorkflow <- workflow(occurrence = CarolinaWrenPO,
                       covariate = CarolinaWrenRasters,
                       process = OneThousandBackground,
                       model = MaxEnt,
                       output = PrintMap(points = FALSE))
```

Now run the exact same `workflow` but switch to a `LogisticRegression` model. How do they compare?

```{r eval=TRUE, message=FALSE, warning=FALSE, fig.align='center', fig.height=7, fig.width=7}
myLogisticflow <- workflow(occurrence = CarolinaWrenPO,
                       covariate = CarolinaWrenRasters,
                       process = OneThousandBackground,
                       model = LogisticRegression,
                       output = PrintMap(points = FALSE))
```

There are some pretty obvious differences in the predicted distribution maps of these two approaches. The only thing to change betweeen the two workflows is our model of choice, so what causes the difference? Check out our more detailed guide on SDM model algorithm selection here **(insert plug for future guides)**


# Conclusion


