---
title: "Introduction to SDMs"
author: "zoontutorials team"
date: "6 February 2017"
output:
  rmarkdown::html_vignette:
    toc: yes
vignette: >
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteIndexEntry{Introduction to SDMs}
  %\VignetteEncoding{UTF-8}
---

# Introduction
* Why do SDMs?
  + maps (prediction) vs. ecology (inference)
* Types of SDMs
  + correlative vs. mechanistic models
  + multispecies models
* brief discussion of the limitations of the method used here
* conceptual map of workflow steps

# Basic zoon SDM Workflow
* This section will include the steps, and the r script, with visuals for the steps (head of data, model formula, output visual)

The zoon workflow is designed to make fitting an SDM straight-forward and reproducible. To fit an SDM you use the `workflow` function, and need only to select a module in each of these five categories: occurrence, covariate, process, model, and output. The following sections of this tutorial will guide you through the process of selecting a module for each argument of the `workflow` function. But first, don't forget to load the package!
```{r message=FALSE, warning=FALSE}
library(zoon)
```
An example workflow, for the mosquito *Anopheles Plumbeus* in the UK, would look like this:  
```{r message=FALSE, warning=FALSE, fig.align='center', fig.height=4, fig.width=3}
example_workflow <- workflow(occurrence = UKAnophelesPlumbeus,
                             covariate = UKAir,
                             process = OneHundredBackground,
                             model = RandomForest,
                             output = PrintMap)
```

`zoon` comes with several pre-existing modules for each argument, so you might like to explore some different combinations to get a feel for how to put a `workflow` together (**Hint:** Try running the `GetModuleList` command).

# Detailed SDM Workflow
## Step 1. Occurrence
* types of occurrence data
  + presence only
  + presence-absence
* limitations of po
* sourced from internet vs. your own data

SDMs are most frequently fit with species occurrence data (rather than, for example, species abundance data), and this can come in three formats: presence-only, presence-background, and presence-absence: 

* Presence-only data is usually sourced from museum or herbarium records, and is just a list of all recorded sightings/captures of a species and its location. 

* Presence-background data is the more commonly used format of presence-only data (rather than an entirely separate format), and combines presence records with a set amount of randomly generated background points or 'pseudo-absences'.

* Presence-absence data generally comes from structured field surveys where the presence *or* absence of the target species is recorded for a given site.

In practice, presence-only data is more commonly used than presence-absence data, so this tutorial is written for presence-only data as default, and any necessary alterations for presence-absence will be noted.

The first module required in a `workflow` is `occurrence` and it is where you load your species occurrence data. There are three methods for loading data into your workflow: using pre-existing `occurrence` modules in `zoon`, downloading data from an online repository, or loading in your own from a local computer. 

### Existing *zoon* data

`zoon` comes with several pre-existing data set modules that you can use, and you can see them using `GetModuleList()`. Under the `$occurrence` sub-heading you can see all available `occurrence` modules including these datasets. For this tutorial, we will be using the `CarolinaWrenPO` module, which is a presence-only dataset of the Carolina Wren in the USA. This is a common resident species in the eastern half of the USA whose range reaches into southern Ontario, Canada and north-east of Mexico, but the dataset has been cropped to the extent of the contiguous USA region. To use this dataset, you would call your `occurrence` with this line of code inside your workflow:

```{r eval=FALSE}
occurrence = CarolinaWrenPO
```

**Add head(data) here?**

### Data from online repositories

To download a species dataset from an online repository such as GBIF you use the `SpOcc` module. When using this module, you need to define several arguments: species, extent, databases, type, and limit. First you need to select your species of interest (by scientific name), then define the extent of the area you are interested in, select a repository, the type of data you are interested in (i.e. presence only), and finally an upper limit to the number of records to be sourced. Several online repositories are available to use [hint: use `ModuleHelp(“SpOcc”)` to see a list of options], but here we use GBIF. We are interested in obtaining presence-only data for the Giant Panda from GBIF, so we set our `occurrence` module like this:

```{r eval=FALSE}
occurrence = SpOcc(species = “Ailuropoda melanoleuca”, extent = “ ”, database = “GBIF”) 
```

** Needs finishing when GBIF is working again to test **

**Add head(data) here?**

### Loading your own data

If you have your own dataset that you want to analyse then you can load it using the `LocalOccurrenceData` module. This requires a .csv file with three columns for the values of longitutude, latitude, and value (i.e. species presence = 1) in that order. For non-latitude/longitude based coordinate systems refer to the information about a CRS column using `ModuleHelp("LocalOccurrenceData")`. To load your own presence-only dataset, set your `occurrence` module within `workflow` as this:

```{r eval=FALSE}
occurrence = LocalOccurrenceData(filename = "myData.csv",
                                 occurrenceType = 'presence')
```

To load your own presence-absence dataset, set your `occurrence` module within `workflow` as this:

```{r eval=FALSE}
occurrence = LocalOccurrenceData(filename = "myData.csv",
                                 occurrenceType = 'presence/absence')
```

**Add example here***

`zoon` has several accessor functions available to view the output of each module in your workflow. Using the `Occurrence()` function you can view your raw species occurrence data. You call this like so:

```{r eval=FALSE, message=FALSE, warning=FALSE}
Occurrence(Your_Workflow_Name_Here)
```

```{r eval=TRUE, echo=FALSE, message=FALSE, warning=FALSE}
hidden_PO_workflow <- workflow(occurrence = CarolinaWrenPO,    ### just some basic workflows so there
                             covariate = CarolinaWrenRasters,  ### is something to run accessor functions
                             process = OneHundredBackground,   ### on as examples
                             model = MaxEnt,
                             output = NoOutput)

hidden_PA_workflow <- workflow(occurrence = CarolinaWrenPA,
                             covariate = CarolinaWrenRasters,
                             process = NoProcess,
                             model = LogisticRegression,
                             output = NoOutput)
```

```{r eval=TRUE, echo=FALSE, message=FALSE, warning=FALSE}
head(Occurrence(hidden_PO_workflow))
```

This is the first six lines of the `CarolinaWrenPO` dataset module. The first two coumns provide the geographic location of the observation (as longitude and latitude), the third column is the observation value (1 = presence, 0 = absence), the fourth column is the type of observation (presence/absence, see below for an example of absence points from the `CarolinaWrenPA` dataset module), and the last column identifies the fold the data point is located in if utilising cross-validation (covered later in the section of `Process` modules, but a default model can be considered to always have one 'fold').

```{r eval=TRUE, echo=FALSE, message=FALSE, warning=FALSE}
tail(Occurrence(hidden_PA_workflow))
```

## Step 2. Covariates
* gridded covariates
* discuss concept of niche space vs. geographic space
  + include a graphical representation of this?
* limitations 
  + mobile species
  + garbage in, garbage out
  + extrapolation

The second module required in a `workflow` is `covariate` and it is where you load in your environmental data. as with the `occurrence` module, there are three ways to load environmental data into your model: use pre-existing `zoon` modules, data from online sources, and loading in your own data from a local computer.

### Existing *zoon* data

The pre-existing `zoon` modules are less exhaustive for `covariates` than `occurrence`, but you still find them using `GetModuleList()`. Under the `$covariate` sub-heading you can see all available `covariate` modules including these datasets. For this tutorial, we will be using the `CarolinaWrenRasters` module. This is the complementary environmental data to our chosen `occurrence` module. To use this dataset, you would call your `covariate` with this line of code inside your workflow:

```{r eval=FALSE}
covariate = CarolinaWrenRasters
```

### Data from online repositories

The three main `covariate` modules for sourcing environmental data from an online repository are `NCEP`, `Bioclim`, and `Bioclim-future`.

* `NCEP`: This module will grab coarse environmental data from the National Centers for Environmental Prediction (NCEP). This requires you to set the coordinates of the model extent and provide a character vector of which variables to select (use `ModuleHelp("NCEP")` to see the options).

```{r eval=FALSE}
covariate = NCEP(extent = c(-5,5,50,60), # default values for the NCEP module
                  variables = "hgt")   
```

* `Bioclim`: This module will grab bioclimatic variables data from WorldClim. This requires you to set the coordinates of the model extent, select a resolution for the data (in minutes), and a vector of integers to identify which bioclim variables to source. Refer to [WorldClim](http://www.worldclim.org/bioclim) for a list of available bioclim variables.

```{r eval=FALSE}
covariate = Bioclim(extent = c(-180,180,-90,90), # default values for the Bioclim module
                     resolution = 10,
                     layers = 1:5)   
```

* `Bioclim_future`: This module will grab biolimatic variables data from WorldClim for the future (CMIP5). This requires you to set the coordinates of the model extent, select a resolution for the data (in minutes), a vector of integers to identify which bioclim variables to source, a [Representative Concentration Pathways trajectory](https://en.wikipedia.org/wiki/Representative_Concentration_Pathways), a General Circulation Model **(need a link!)**, a time period for the projection ('50' for the period 2041-2060, or '70' for 2061-2080).

```{r eval=FALSE}
covariate = Bioclim_future(extent = c(-10, 10, 45, 65),   # default values for the Bioclim_future module
                            resolution = 10,
                            layers = 1:19,
                            rcp = 45,
                            model = "AC",
                            year = 70)
```

### Loading your own data

You can load your own environmental data into your `workflow` uing the `LocalRaster` module. This will load in either a single raster or a list of rasters and create a raster stack dependant on what you define. This can take the form of a string of the filename of the raster layer, a list/vector of strings of filenames of rasters to be stacked, a RasterLayer or a RasterStack object. The structure of this module call is as follows:

```{r eval=FALSE}
covariate = LocalRaster(rasters = "MyRaster")
```

**Add head(data) here?**

Using the `Covariate()` accessor function you can see the details of the raster stack that has been loaded/created. You can use the `?'RasterStack-class'` command to find a more explicit summary, but key thins to look for include the extent of the dataset, coordinate projection scheme (here, WGS84), and minimum/maximum value for your variables. You can call this with the following command:

```{r eval=FALSE}
Covariate(Your_Workflow_Name_Here)
```

```{r eval=TRUE, echo=FALSE, message=FALSE, warning=FALSE}
Covariate(hidden_PO_workflow)
```

## Step 3. Process

Now that you have loaded in your data using the `occurrence` and `covariate` modules, the `process` modules will perform any processes required before fitting the model itself. These modules can be broadly classified into those that effect your data, those that generate additional data, or those that set up model validation.

### Process your data

The `process` modules that effect your data allow you to clean your raw data, and to process it in such a manner as to improve the interpretability of your results.

*	Incomplete or incorrect data can lead to drawing false conclusions, so the `Clean` module performs data cleansing to remove impossible, incomplete, or unlikely occurrence points in a data set based on the longitude/latitude values. Use `ModuleHelp("Clean")` to see how to select a cleaning method.

```{r eval=FALSE}
process = Clean(which = c(1,2,3,4))   # default fit of the Clean module
```

*	The `StandardiseCov` module standardises selected variables (by default, all of them) by subtracting the mean and dividing by the standard deviation, which allows for the direct comparison of the effect size of variables. This vastly improves the interpretability of the model, as variables measured at different scales will often lead to difficult comparisons of regression coefficients (aka effect size). For example, the regression coefficient for the distance of a site to roads might be +3.0 when measured in kilometres, but if measured in meters it would be +0.003, and then for effect of average temperature in Celsius could be -10. How do you compare the effect of these variables? Standardisation of data is especially useful for the comparison of coefficients within a model.

To use this module you need to choose which, if any, variables you want to exclude from standardisation, and if you want to use the Gelman variant of standardissing by two standard deviations instead of one **(link to this here)**.

```{r eval=FALSE}
process = StandardiseCov(Gelman = FALSE, exclude = NULL)   # default form of the StandardiseCov module.
```

*	The `Transform` module lets you apply a transformation function (e.g. square, log, etc) to your environmental variables. This requires you to define the transformation, a character vector listing the covariates to be transformed, and whether you want to replace the original variable with the transformed version or add it as an extra layer.

```{r eval=FALSE}
process = Transform(trans = function(x) {x},   # default form of the Transform module. Perform no transformations on any variable
                    which_cov = NULL,
                    replace = TRUE)
```

```{r eval=FALSE}
process = Transform(trans = function(x) {x^2},   # Add a new variable that is the squared transformation of the "cov1" variable
                    which_cov = "cov1",
                    replace = FALSE)
```

```{r eval=FALSE}
process = Transform(trans = function(x) {log(x)}, # Replace the "cov1" variable with the log-transformed version of "cov1"
                    which_cov = "cov1",
                    replace = TRUE)
```

### Generate new data

The `process` modules that generate new data include the generation of background data points required for models being fit to presence-only data, or the creation of interactions between variables in the model.

All models that utilise presence-only data sets **(or almost all? at least within zoon)** require the generation of background data points (aka pseudo-absences). The data points represent the range of environmental conditions present in the region being modelled. There are several modules in zoon that undertake this process: 

* `Background` generates n background samples (default 100). You need to define the number of background samples you wish to generate, and have the option of defining a bias value (see `ModuleHelp("Background")`).

```{r eval=FALSE}
process = Background(n = 100,   # generate 100 unbiased background points
                     bias = NULL)
```

* `OneHundredBackground` and `OneThousandBackground` generate preset quantities (100 and 1000 points respectively)

```{r eval=FALSE}
process = OneHundredBackground
```

```{r eval=FALSE}
process = OneThousandBackground
```

Interactions between variables can have important implications for the interpretation of statistical models. Expand when back in front of textbooks (Something about interpretation of response curves? Doesn't effect maps). The `addInteraction` module lets you define interactions between variables in your model. This is not generating new data per se, but adds additional variables to the model based on existing variables. There are three ways to implement the `addInteractions` module:

* You can add all pair-wise interactions:
```{r eval=FALSE}
process = addInteractions(which.covs = 'pairs')
```

* You can add all interactions between a select group of variables:
```{r eval=FALSE}
process = addInteractions(which.covs = c(A,B))   # adds an interacton between A & B

process = addInteractions(which.covs = list(c(A,B), c(A,C)))   # adds interactions between A & B and A & C, but not B & C

process = addInteractions(which.covs = c(A,B,C))   # adds all two-way (e.g. A & B) interactions and a three-way interaction between A, B & C
```

* You can specify polynomial terms:
```{r eval=FALSE}
process = addInteractions(which.covs = c(A,A))   # leads to a quadratic polynomial

process = addInteractions(which.covs = c(A,A,A))   # leads to a cubic, polynomial
```

### Set up model validation

Model validation is a very important step in the model fitting process as it lets you determine if your model is an acceptable description of your data. Model validation can be as simple as including goodnss-of-fit tests or analysing the residuals **can this be done for SDMs?** but the `zoon` package uses more complex methods that subset and/or resample the dataset. **[Expand on what model validation is/why it is important]** The Process modules that set up model validation include:

*	`Crossvalidate`: Run k fold crossvalidation (default = 5). If you are using presence-absence, this will split presences and absences separately so folds have equally balanced data, otherwise it just samples. This samples the dataset without replacement into k 'folds' or sub-datasets to be used for training and testing the model. A model is fit to the k-1 folds of training data and evaluated against the fold of testing data (and repeated so each fold is used as the testing data once, and then the results are averaged). This is mainly used where the goal is to use the model for prediction as it tests if the model is overfit to the training data and thus unlikely to be suitable for prediction across another dataset (i.e. the test fold). 

```{r eval=FALSE}
process = Crossvalidate(k = 5)   # 5-fold cross-validation
```

*	`Bootstrap`: This module generates random bootstraps of the data. **expand** It defaults to the number of points in the dataset

```{r eval=FALSE}
process = Bootstrap   # Bootstrap a number of points equal to the number of points in the dataset
```

*	`BackgroundAndCrossvalid`: This module generates up to 100 background records at random in the raster cells and splits all data in k folds for cross validation (default = 5).

```{r eval=FALSE}
process = BackgroundAndCrossvalid(k = 5)   # generate up to 100 background points, and the split all data into 5 folds for cross validation
```

In some instances you may not require a `process` module in your `workflow`,  however, it is a mandatory argument and so the `NoProcess` module can be used as a 'blank' module.

```{r eval=FALSE}
process = NoProcess
```

## Step 4. Model

Arguably the most important part of your `workflow`, is the `model` module. This is where you choose which type of SDM you want to fit to your data. There is a multitude of different SDM methods to choose from **(add Elith paper reference)** and each with its own merits, but here we provide a quick overview of some of the more common methods. 

### Logistic Regression

Logistic regression, fit with the `LogisticRegression` module, is a type of Generalised Linear Model for analysing datasets where the outcome is a dichotomous variable (i.e. only two possible outcomes, like the presence or absence of a species, represented binarily as 1/0), and the outcome is dependent on one or more independent variables (i.e. your environmental data). Coefficient estimation is achieved by choosing parameter values that maximise the likelihood of observing the sample values. This model can be fit to presence-background and presence-absence datasets. You select this model by defining the model in your `workflow` as the following (no arguments required):

```{r eval=FALSE}
model = LogisticRegression
```

### mgcv

The `mgcv` module fits a Generalises Additive Model (GAM) using generalised cross-validation via the `mgcv` package. In contrast to GLMs (like the `LogisticRegression` module) where the coefficient for each variable is estimated, in a GAM the linear predictor is the sum of smoothing functions fit to each measured variable. **(Add definition of smoothing functions?)**. This model can be fit to presence-background and presence-absence datasets. To fit this module you need to define a basis dimension used to represent the smooth term, and a two-letter character string indicating the smoothing basis to use. You select this model in your `workflow` using the following call:

```{r eval=FALSE}
model = mgcv(k = -1,   # default settings
             bs = "tp")
```

### MaxEnt

The `MaxEnt` module fits a MaxEnt model using the `maxent()` function in the `dismo` package. MaxEnt is the most widely used SDM algorithm **(Elith reference)** today. This is a machine learning method that computes a probability distribution over the environment data grid cells, and the chosen distribution is the one that maximises the entropy (hence, MaxEnt) subject to some constraints. This method works with presence-background data. This module does require you to have the MaxEnt executable file saved in the corret location. The `zoon` helper function `GetMaxEnt()` is available to help with this installation. You select this model in your `workflow` using the following call:

```{r eval=FALSE}
model = MaxEnt
```

### Boosted Regression Trees

The `GBM` module fits a generalised boosted regression model (aka boosted regression trees) using the `gbm` package. Boosted regression trees (BRTs) are known by a wide variety of names (such as Gradient Boosting Machine, hence 'GBM'), but this is the name most commonly used when referring to species distribution models. This is a machine learning technique that produces a prediction model in the form of an ensemble of weak prediction models (i.e. decision trees). This differs from the standard regression approach of fitting a single best model by using the 'boosting' technique to combine combine relatively large numbers of simple trees adaptively, to optimise predictive performance. For this model you need to define the maximum number of trees, the maximum depth of variable interactions, and a shrinkage parameter (aka the learning rate). This model can be fit to presence-background and presence-absence data using the following call in your `workflow`:

```{r eval=FALSE}
model = GBM(max.trees = 1000,   # default values
            interaction.depth = 5,
            shrinkage = 0.001)
```

### RandomForest

The `RandomForest` module fits a random forest classification model. Similarly to the boosted regression trees in the `GBM` module, random forests are a machine learning technique that make use of an ensemble of weak prediction models (i.e. decision trees). Random forests differ from BRTs by fitting each decision tree independently from previously fit trees using bootstrapped samples of your data. **(need a better definition of the difference between BRT and RF?)**. This model can be fit to presence-background or presence-absence data using the following call in your `workflow`:

```{r eval=FALSE}
model = RandomForest
```

## Step 5. Output
* types of output
* interpreting outputs

## Putting it all together

Now that we have seen how to use each argument in the `workflow` function it is time to put all of the pieces together and fit our own SDM within `zoon`. Since it is the most popular SDM methodology, lets fit a MaxEnt model to our Carolina Wren presence-background data set, generate 1000 background samples, and display our results in a map without our datapoints displayed. We have covered each of the necesssary modules previously, but when we run them together we get a complete `workflow`.

```{r eval=TRUE, message=FALSE, warning=FALSE, fig.align='center', fig.height=7, fig.width=7}
myMaxEntWorkflow <- workflow(occurrence = CarolinaWrenPO,
                       covariate = CarolinaWrenRasters,
                       process = OneThousandBackground,
                       model = MaxEnt,
                       output = PrintMap(points = FALSE))
```

Now run the exact same `workflow` but switch to a `LogisticRegression` model. How do they compare?

```{r eval=TRUE, message=FALSE, warning=FALSE, fig.align='center', fig.height=7, fig.width=7}
myLogisticflow <- workflow(occurrence = CarolinaWrenPO,
                       covariate = CarolinaWrenRasters,
                       process = OneThousandBackground,
                       model = LogisticRegression,
                       output = PrintMap(points = FALSE))
```

There are some pretty obvious differences in the predicted distribution maps of these two approaches. The only thing to change betweeen the two workflows is our model of choice, so what causes the difference? Check out our more detailed guide on SDM model algorithm selection here **(insert plug for future guides)**


# Conclusion


